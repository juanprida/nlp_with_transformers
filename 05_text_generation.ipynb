{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\e10115582\\miniconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing GT2\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# We load the model.\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In an old city full of castles and penguins\"\n",
    "# We tokenize the text\n",
    "tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In an old city full of castles and penguins'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output = tokenizer.decode(tokenized_text[0], skip_special_tokens=True)\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In an old city full of castles and penguins, this is probably where\n"
     ]
    }
   ],
   "source": [
    "# We generate the output.\n",
    "output = model.generate(tokenized_text, max_length=15, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=3)\n",
    "\n",
    "# We decode the output.\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy search.\n",
    "- Let's implement the method `generate` by selecting the word with the highest probability at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy_search(model, tokenizer, text, max_length):\n",
    "    \"\"\"\n",
    "    Generates text using greedy search.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: GPT2LMHeadModel\n",
    "        The model to use for generating text.\n",
    "    tokenizer: GPT2Tokenizer\n",
    "        The tokenizer to use for generating text.\n",
    "    text: str\n",
    "        The text to use as a starting point for generating text.\n",
    "    max_length: int\n",
    "        The maximum length of the generated text.\n",
    "    \"\"\"\n",
    "    # Tokenize the text.\n",
    "    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits = model(tokenized_text)[0]\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        tokenized_text = torch.cat([tokenized_text, next_token.unsqueeze(0)], dim=-1)\n",
    "    # Decode the tokenized text.\n",
    "    decoded_output = tokenizer.decode(tokenized_text[0], skip_special_tokens=True)\n",
    "    return decoded_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beam_search(model, tokenizer, text, max_length, beam_size=5):\n",
    "    \"\"\"\n",
    "    Generates text using beam search.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: GPT2LMHeadModel\n",
    "        The model to use for generating text.\n",
    "    tokenizer: GPT2Tokenizer\n",
    "        The tokenizer to use for generating text.\n",
    "    text: str\n",
    "        The text to use as a starting point for generating text.\n",
    "    max_length: int\n",
    "        The maximum length of the generated text.\n",
    "    beam_size: int, default 5\n",
    "        The beam size for the search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A list of the top k generated texts with their respective scores.\n",
    "    \"\"\"\n",
    "    # Tokenize the text.\n",
    "    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Initialize the beam search.\n",
    "    beam = [{\"tokens\": tokenized_text, \"score\": 0.0}]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Create a list to store the new candidates for the beam.\n",
    "        candidates = []\n",
    "\n",
    "        # Generate candidates for each element in the current beam.\n",
    "        for elem in beam:\n",
    "            # Get the last token of the current candidate.\n",
    "            last_token = elem[\"tokens\"][:, -1:]\n",
    "\n",
    "            # Get the logits for the next token using the model.\n",
    "            logits = model(elem[\"tokens\"])[0][:, -1, :]\n",
    "\n",
    "            # Apply a log softmax to the logits.\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "            # Calculate the scores of the next tokens by adding the log probabilities to the scores of the current candidate.\n",
    "            scores = log_probs + elem[\"score\"]\n",
    "\n",
    "            # Get the top k scores and their corresponding tokens.\n",
    "            topk_scores, topk_tokens = torch.topk(scores, k=beam_size, dim=-1)\n",
    "\n",
    "            # Create new candidates for each of the top k tokens.\n",
    "            for i in range(beam_size):\n",
    "                candidate = {}\n",
    "                candidate[\"tokens\"] = torch.cat([elem[\"tokens\"], topk_tokens[:, i:i+1]], dim=-1)\n",
    "                candidate[\"score\"] = topk_scores[:, i].item()\n",
    "                candidates.append(candidate)\n",
    "\n",
    "        # Sort the candidates by score and keep the top k.\n",
    "        candidates = sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:beam_size]\n",
    "\n",
    "        # Check if the top candidate has an end-of-text token.\n",
    "        top_candidate_tokens = candidates[0][\"tokens\"]\n",
    "        if top_candidate_tokens[0, -1] == tokenizer.eos_token_id:\n",
    "            # If so, return the generated text.\n",
    "            generated_text = tokenizer.decode(top_candidate_tokens[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "\n",
    "        # Update the beam with the new candidates.\n",
    "        beam = candidates\n",
    "\n",
    "    # Return the top k generated texts.\n",
    "    generated_texts = [(tokenizer.decode(candidate[\"tokens\"][0], skip_special_tokens=True), candidate[\"score\"]) for candidate in beam]\n",
    "\n",
    "    return generated_texts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search with sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beam_search_sampling(model, tokenizer, text, max_length, beam_size=5, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates text using beam search with sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: GPT2LMHeadModel\n",
    "        The model to use for generating text.\n",
    "    tokenizer: GPT2Tokenizer\n",
    "        The tokenizer to use for generating text.\n",
    "    text: str\n",
    "        The text to use as a starting point for generating text.\n",
    "    max_length: int\n",
    "        The maximum length of the generated text.\n",
    "    beam_size: int, default 5\n",
    "        The beam size for the search.\n",
    "    temperature: float, default 1.0\n",
    "        The temperature value to use for sampling. Higher values result in more diverse samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A list of the top k generated texts with their respective scores.\n",
    "    \"\"\"\n",
    "    # Tokenize the text.\n",
    "    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Initialize the beam search.\n",
    "    beam = [{\"tokens\": tokenized_text, \"score\": 0.0}]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Create a list to store the new candidates for the beam.\n",
    "        candidates = []\n",
    "\n",
    "        # Generate candidates for each element in the current beam.\n",
    "        for elem in beam:\n",
    "            # Get the last token of the current candidate.\n",
    "            last_token = elem[\"tokens\"][:, -1:]\n",
    "\n",
    "            # Get the logits for the next token using the model.\n",
    "            logits = model(elem[\"tokens\"])[0][:, -1, :]\n",
    "\n",
    "            # Apply a softmax with temperature to the logits.\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            # Sample a token from the distribution.\n",
    "            next_token = torch.multinomial(probs, num_samples=beam_size)\n",
    "\n",
    "            # Calculate the score of the next token by adding the log probability to the score of the current candidate.\n",
    "            scores = torch.log(probs[0, next_token[0]]).item() + elem[\"score\"]\n",
    "\n",
    "            # Get the top k scores and their corresponding tokens.\n",
    "            topk_scores, topk_tokens = torch.topk(scores, k=beam_size, dim=-1)\n",
    "\n",
    "\n",
    "            # Create a new candidate with the sampled token and score.\n",
    "            candidate = {}\n",
    "            candidate[\"tokens\"] = torch.cat([elem[\"tokens\"], topk_tokens[:, i:i+1]], dim=-1)\n",
    "            candidate[\"score\"] = score\n",
    "            candidates.append(candidate)\n",
    "\n",
    "        # Sort the candidates by score and keep the top k.\n",
    "        candidates = sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:beam_size]\n",
    "        print(len(candidates))\n",
    "\n",
    "        # Check if the top candidate has an end-of-text token.\n",
    "        top_candidate_tokens = candidates[0][\"tokens\"]\n",
    "        if top_candidate_tokens[0, -1] == tokenizer.eos_token_id:\n",
    "            # If so, return the generated text.\n",
    "            generated_text = tokenizer.decode(top_candidate_tokens[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "\n",
    "        # Update the beam with the new candidates.\n",
    "        beam = candidates\n",
    "\n",
    "    # Return the top k generated texts.\n",
    "    generated_texts = [(tokenizer.decode(candidate[\"tokens\"][0], skip_special_tokens=True), candidate[\"score\"]) for candidate in beam]\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\e10115582\\PycharmProjects\\nlp_with_transformers\\05_text_generation.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/e10115582/PycharmProjects/nlp_with_transformers/05_text_generation.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generate_beam_search_sampling(model, tokenizer, \u001b[39m\"\u001b[39;49m\u001b[39mThe meditation is a \u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m20\u001b[39;49m, beam_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\e10115582\\PycharmProjects\\nlp_with_transformers\\05_text_generation.ipynb Cell 13\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/e10115582/PycharmProjects/nlp_with_transformers/05_text_generation.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m next_token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmultinomial(probs, num_samples\u001b[39m=\u001b[39mbeam_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/e10115582/PycharmProjects/nlp_with_transformers/05_text_generation.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Calculate the score of the next token by adding the log probability to the score of the current candidate.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/e10115582/PycharmProjects/nlp_with_transformers/05_text_generation.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlog(probs[\u001b[39m0\u001b[39;49m, next_token[\u001b[39m0\u001b[39;49m]])\u001b[39m.\u001b[39;49mitem() \u001b[39m+\u001b[39m elem[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/e10115582/PycharmProjects/nlp_with_transformers/05_text_generation.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Get the top k scores and their corresponding tokens.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/e10115582/PycharmProjects/nlp_with_transformers/05_text_generation.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m topk_scores, topk_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(scores, k\u001b[39m=\u001b[39mbeam_size, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "generate_beam_search_sampling(model, tokenizer, \"The meditation is a \", 20, beam_size=5, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34c8409de71f4f7c4cf0d5fe0f549cd4a6baa74feee52e1801ba3d4749f24fab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
